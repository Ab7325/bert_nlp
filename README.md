# bert_nlp

The notebook bert_nlp.ipynb evaluates BERT, which stands for Bidirectional Encoder Representations from Transformers, was a ground-breaking model introduced in 2018 by Google.
It is used for predicting missing words in sequences and also for question-answer reasoning by returning a span from the context sentence given as per the question. Both the uses have been evaluated in the notebook 
